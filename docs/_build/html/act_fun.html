

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Activation Functions &mdash; cvnn 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Initializers" href="initializers.html" />
    <link rel="prev" title="Complex Dropout" href="layers/others.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> cvnn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Complex Layers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Activation Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#complex-input-real-output">Complex input, real output</a></li>
<li class="toctree-l2"><a class="reference internal" href="#type-a-cartesian-form">TYPE A: Cartesian form</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classification">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#type-b-polar-form">TYPE B: Polar form</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="initializers.html">Initializers</a></li>
<li class="toctree-l1"><a class="reference internal" href="montecarlo.html">Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="code_examples.html">Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About Me</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">cvnn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Activation Functions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/act_fun.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="activation-functions">
<h1>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h1>
<p id="activations">There are two ways to use an activation function</p>
<p>Option 1: Using the string as in <code class="code docutils literal notranslate"><span class="pre">act_dispatcher</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ComplexDense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;cart_sigmoid&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Option 2: Using the function directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cvnn.activations</span> <span class="kn">import</span> <span class="n">cart_sigmoid</span>

<span class="n">ComplexDense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">cart_sigmoid</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless explicitedly said otherwise, these activation functions does not change the input dtype.</p>
</div>
<p>List of activation functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">act_dispatcher</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">linear</span><span class="p">),</span>
    <span class="c1"># Complex input, Real output</span>
    <span class="s1">&#39;convert_to_real_with_abs&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">convert_to_real_with_abs</span><span class="p">),</span>
    <span class="s1">&#39;softmax_real&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">softmax_real</span><span class="p">),</span>
    <span class="c1"># Cartesian form</span>
    <span class="s1">&#39;cart_sigmoid&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_sigmoid</span><span class="p">),</span>
    <span class="s1">&#39;cart_elu&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_elu</span><span class="p">),</span>
    <span class="s1">&#39;cart_exponential&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_exponential</span><span class="p">),</span>
    <span class="s1">&#39;cart_hard_sigmoid&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_hard_sigmoid</span><span class="p">),</span>
    <span class="s1">&#39;cart_relu&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_relu</span><span class="p">),</span>
    <span class="s1">&#39;cart_leaky_relu&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_leaky_relu</span><span class="p">),</span>
    <span class="s1">&#39;cart_selu&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_selu</span><span class="p">),</span>
    <span class="s1">&#39;cart_softplus&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_softplus</span><span class="p">),</span>
    <span class="s1">&#39;cart_softsign&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_softsign</span><span class="p">),</span>
    <span class="s1">&#39;cart_tanh&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_tanh</span><span class="p">),</span>
    <span class="s1">&#39;cart_softmax&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">cart_softmax</span><span class="p">),</span>
    <span class="c1"># Polar form</span>
    <span class="s1">&#39;pol_tanh&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">pol_tanh</span><span class="p">),</span>
    <span class="s1">&#39;pol_sigmoid&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">pol_sigmoid</span><span class="p">),</span>
    <span class="s1">&#39;pol_selu&#39;</span><span class="p">:</span> <span class="n">Activation</span><span class="p">(</span><span class="n">pol_selu</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To define your own activation function, just create it inside the <code class="code docutils literal notranslate"><span class="pre">activations.py</span></code> and add it to <code class="code docutils literal notranslate"><span class="pre">act_dispatcher</span></code> dictionary at the end</p>
</div>
<dl class="py method">
<dt id="linear">
<code class="sig-name descname">linear</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#linear" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Does not apply any activation function. It just outputs the input.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor variable</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>z</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="complex-input-real-output">
<h2>Complex input, real output<a class="headerlink" href="#complex-input-real-output" title="Permalink to this headline">¶</a></h2>
<dl class="py method">
<dt id="softmax_real">
<code class="sig-name descname">softmax_real</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#softmax_real" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax">softmax</a> function to the modulus of z.
The softmax activation function transforms the outputs so that all values are in range (0, 1) and sum to 1.
It is often used as the activation for the last layer of a classification network because the result could be
interpreted as a probability distribution.
The softmax of x is calculated by:</p>
<div class="math notranslate nohighlight">
\[\frac{e^x}{\textrm{tf.reduce_sum}(e^x)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Real-valued tensor of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="convert_to_real_with_abs">
<code class="sig-name descname">convert_to_real_with_abs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#convert_to_real_with_abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the absolute value and returns a real-valued output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Real-valued tensor of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following <code class="code docutils literal notranslate"><span class="pre">cart</span></code> or <code class="code docutils literal notranslate"><span class="pre">pol</span></code> means either type A (cartesian) or type B (polar) according to <a class="reference internal" href="#cit2003-kuroe" id="id1"><span>[CIT2003-KUROE]</span></a> notation.</p>
</div>
</div>
<div class="section" id="type-a-cartesian-form">
<h2>TYPE A: Cartesian form<a class="headerlink" href="#type-a-cartesian-form" title="Permalink to this headline">¶</a></h2>
<dl class="py method">
<dt id="cart_sigmoid">
<code class="sig-name descname">cart_sigmoid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Called with <code class="code docutils literal notranslate"><span class="pre">'cart_sigmoid'</span></code> string.</p>
<p>Applies the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid">sigmoid</a> function to both the real and imag part of z.</p>
<div class="math notranslate nohighlight">
\[\frac{1}{1 + e^{-x}} + j  \frac{1}{1 + e^{-y}}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[z = x + j y\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Tensor to be used as input of the activation function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_elu">
<code class="sig-name descname">cart_elu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_elu" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu">Exponential linear unit</a>. To both the real and imaginary part of z.</p>
<div class="math notranslate nohighlight">
\[x if x &gt; 0 and alpha * (exp(x)-1) if x &lt; 0\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z</strong> – Input tensor.</p></li>
<li><p><strong>alpha</strong> – A scalar, slope of negative section.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_exponential">
<code class="sig-name descname">cart_exponential</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_exponential" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Exponential activation function. Applies to both the real and imag part of z the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/exponential">exponential activation</a>:</p>
<div class="math notranslate nohighlight">
\[e^x\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_hard_sigmoid">
<code class="sig-name descname">cart_hard_sigmoid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_hard_sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/hard_sigmoid">hard Sigmoid</a> function to both the real and imag part of z.</p>
</div></blockquote>
<p>The hard sigmoid function is faster to compute than sigmoid activation.
Hard sigmoid activation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}            0               ,\quad x &lt; -2.5 \\
1               ,\quad x &gt; 2.5 \\
0.2 * x + 0.5   ,\quad -2.5 &lt;= x &lt;= 2.5\end{split}\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_relu">
<code class="sig-name descname">cart_relu</code><span class="sig-paren">(</span><em class="sig-param">z</em>, <em class="sig-param">alpha=0.0</em>, <em class="sig-param">max_value=None</em>, <em class="sig-param">threshold=0</em><span class="sig-paren">)</span><a class="headerlink" href="#cart_relu" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu">Rectified Linear Unit</a> to both the real and imag part of z.</p>
</div></blockquote>
<p>The relu function, with default values, it returns element-wise max(x, 0).</p>
<p>Otherwise, it follows:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}            f(x) = \textrm{max_value}, \quad \textrm{for} \quad x &gt;= \textrm{max_value} \\
f(x) = x, \quad \textrm{for} \quad \textrm{threshold} &lt;= x &lt; \textrm{max_value} \\
f(x) = \alpha * (x - \textrm{threshold}), \quad \textrm{otherwise} \\\end{split}\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_leaky_relu">
<code class="sig-name descname">cart_leaky_relu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu">Leaky Rectified Linear Unit</a> <a class="reference internal" href="#cit2013-maas" id="id2"><span>[CIT2013-MAAS]</span></a> (<a class="reference external" href="http://robotics.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">source</a>) to both the real and imag part of z.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z</strong> – Input tensor.</p></li>
<li><p><strong>alpha</strong> – Slope of the activation function at x &lt; 0. Default: 0.2</p></li>
<li><p><strong>name</strong> – A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_selu">
<code class="sig-name descname">cart_selu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu">Scaled Exponential Linear Unit (SELU)</a> <a class="reference internal" href="#cit2017-klambauer" id="id3"><span>[CIT2017-KLAMBAUER]</span></a> (<a class="reference external" href="https://arxiv.org/abs/1706.02515">source</a>) to both the real and imag part of z.</p>
<p>The scaled exponential unit activation:</p>
<div class="math notranslate nohighlight">
\[\textrm{scale} * \textrm{elu}(x, \alpha)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt>
<code class="sig-name descname">cart_softplus(z):</code></dt>
<dd><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softplus">Softplus</a> activation function to both the real and imag part of z.
The Softplus function:</p>
<div class="math notranslate nohighlight">
\[log(e^x + 1)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt>
<code class="sig-name descname">cart_softsign(z):</code></dt>
<dd><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softsign">Softsign</a> activation function to both the real and imag part of z.
The softsign activation:</p>
<div class="math notranslate nohighlight">
\[\frac{x}{\lvert x \rvert + 1}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cart_tanh">
<code class="sig-name descname">cart_tanh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_tanh" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh">Hyperbolic Tangent</a> (tanh) activation function to both the real and imag part of z.</p>
</div></blockquote>
<p>The tanh activation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}.\]</div>
</div></blockquote>
<p>The derivative if tanh is computed as  <span class="math notranslate nohighlight">\(1 - tanh^2\)</span> so it should be fast to compute for backprop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<dl class="py method">
<dt id="cart_softmax">
<code class="sig-name descname">cart_softmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cart_softmax" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Applies the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax">softmax function</a> to both the real and imag part of z.</p>
</div></blockquote>
<p>The softmax activation function transforms the outputs so that all values are in range (0, 1) and sum to 1.
It is often used as the activation for the last layer of a classification network because the result could be
interpreted as a probability distribution.
The softmax of x is calculated by:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{e^x}{\textrm{tf.reduce_sum}(e^x)}\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="type-b-polar-form">
<h2>TYPE B: Polar form<a class="headerlink" href="#type-b-polar-form" title="Permalink to this headline">¶</a></h2>
<dl class="py method">
<dt id="pol_selu">
<code class="sig-name descname">pol_selu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">z</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pol_selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu">Scaled Exponential Linear Unit (SELU)</a> <a class="reference internal" href="#cit2017-klambauer" id="id6"><span>[CIT2017-KLAMBAUER]</span></a> (<a class="reference external" href="https://arxiv.org/abs/1706.02515">source</a>) to the absolute value of z, keeping the phase unchanged.</p>
<p>The scaled exponential unit activation:</p>
<div class="math notranslate nohighlight">
\[\textrm{scale} * \textrm{elu}(x, \alpha)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>z</strong> – Input tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor result of the applied activation function</p>
</dd>
</dl>
</dd></dl>

<dl class="citation">
<dt class="label" id="cit2003-kuroe"><span class="brackets"><a class="fn-backref" href="#id1">CIT2003-KUROE</a></span></dt>
<dd><p>Kuroe, Yasuaki, Mitsuo Yoshid, and Takehiro Mori. “On activation functions for complex-valued neural networks—existence of energy functions—.” Artificial Neural Networks and Neural Information Processing—ICANN/ICONIP 2003. Springer, Berlin, Heidelberg, 2003. 985-992.</p>
</dd>
<dt class="label" id="cit2013-maas"><span class="brackets"><a class="fn-backref" href="#id2">CIT2013-MAAS</a></span></dt>
<dd><ol class="upperalpha simple">
<li><ol class="upperalpha simple" start="12">
<li><p>Maas, A. Y. Hannun, and A. Y. Ng, “Rectifier Nonlinearities Improve Neural Network Acoustic Models,” 2013.</p></li>
</ol>
</li>
</ol>
</dd>
<dt class="label" id="cit2017-klambauer"><span class="brackets">CIT2017-KLAMBAUER</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><ol class="upperalpha simple" start="7">
<li><p>Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-Normalizing Neural Networks,” ArXiv170602515 Cs Stat, Sep. 2017. Available: <a class="reference external" href="http://arxiv.org/abs/1706.02515">http://arxiv.org/abs/1706.02515</a>.</p></li>
</ol>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="initializers.html" class="btn btn-neutral float-right" title="Initializers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="layers/others.html" class="btn btn-neutral float-left" title="Complex Dropout" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, J Agustin BARRACHINA

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>